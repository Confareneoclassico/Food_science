{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "from functools import lru_cache\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pkg_resources\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sobol_seq\n",
    "import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package requirements for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            \n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "            \n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the initial dataset you'll be working on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SatelliteJuly = pd.read_excel('Data.xlsx',sheet_name='July_sat').drop([r for r in range(12,24)])\n",
    "SatelliteJune = pd.read_excel('Data.xlsx',sheet_name='June_sat').drop([r for r in range(12,24)])\n",
    "SatelliteMay = pd.read_excel('Data.xlsx',sheet_name='May_sat').drop([r for r in range(12,24)])\n",
    "SatelliteApril = pd.read_excel('Data.xlsx',sheet_name='April_satellite').drop([r for r in range(12,\n",
    "                                                                                                24)])\n",
    "Dough = pd.read_excel('Data.xlsx',sheet_name='Dough',usecols=[2,3]).drop([r for r in range(12,24)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD = [SatelliteApril,SatelliteMay,SatelliteJune,SatelliteJuly]\n",
    "\n",
    "SatelliteData = pd.concat(SD,axis=1,sort=False).dropna(axis=1)\n",
    "SatelliteData.index = SatelliteData.index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DoughDistributions = pd.read_excel('alvrografosensit.xls',header=1,index_col=0,usecols=[0,1,2,3,4]).drop([r for r in range(13,\n",
    "                                                                                                25)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the (quasi)-random sample out of the distribution of features measured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = pd.DataFrame(sobol_seq.i4_sobol_generate(6,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_dist = pd.concat([seed[0]*pm for pm in (DoughDistributions.groupby('campione').P.max()-\n",
    "                                           DoughDistributions.groupby('campione').P.min())]).astype(int)\n",
    "L_dist = pd.concat([seed[1]*pm for pm in (DoughDistributions.groupby('campione').L.max()-\n",
    "                                           DoughDistributions.groupby('campione').L.min())]).astype(int)\n",
    "W_dist = pd.concat([seed[2]*pm for pm in (DoughDistributions.groupby('campione').P.max()-\n",
    "                                           DoughDistributions.groupby('campione').P.min())]).astype(int)\n",
    "Features_dist = pd.concat([P_dist,L_dist,W_dist],axis=1)\n",
    "rng = [dd for dd in range(1,13) for se in range(len(seed))]\n",
    "rng.extend([dd for dd in range(25,49) for se in range(len(seed))])\n",
    "\n",
    "Features_dist.index = rng\n",
    "Features_dist = Features_dist.rename(columns={0:'P',1:'L',2:'W'})\n",
    "Features_dist = Features_dist + DoughDistributions[['P','L','W']].groupby('campione').min()\n",
    "Features_dist['P_L'] = Features_dist.P/Features_dist.L\n",
    "Fd = Features_dist.iloc[:,-2:]\n",
    "\n",
    "SatelliteData=SatelliteData.reindex(Fd.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the random-forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(n_estimators=500,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_predictions = []\n",
    "for row in np.unique(SatelliteData.index):\n",
    "    regr.fit(SatelliteData.drop(row),Fd.drop(row))\n",
    "    \n",
    "    l_predictions.append(regr.predict(SatelliteData.loc[row].iloc[0].values.reshape(1,-1))[0])\n",
    "    \n",
    "df_predictions = pd.DataFrame([l for l in l_predictions],columns=Fd.columns,index=np.unique(SatelliteData.index))    \n",
    "\n",
    "distance = Fd - df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_w = pd.concat([g.reset_index(drop=True) for index,g in distance.groupby(by=distance.index).W],axis=1)\n",
    "distance_pl = pd.concat([g.reset_index(drop=True) for index,g in distance.groupby(by=distance.index).P_L],axis=1)\n",
    "distance_w.columns = np.unique(SatelliteData.index)\n",
    "distance_pl.columns = np.unique(SatelliteData.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "distance_w.boxplot()\n",
    "plt.title('W_distance_across_samples')\n",
    "plt.savefig('W_distance_across_samples.png')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "distance_pl.boxplot()\n",
    "plt.title('P/L_distance_across_samples')\n",
    "plt.savefig('P_L_distance_across_samples.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
